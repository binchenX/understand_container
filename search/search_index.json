{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Understand Container","text":"<p>Nothing here. Click the chapters in the left side to dig in.</p>"},{"location":"ch1/","title":"OCI Specification","text":"<p>The Open Container Initiative (OCI) is a collaborative industry effort to define open container specifications, including container image format and runtime. The history of its evolution, from initial disagreements to its current state, is a fascinating story of collaboration and competition in the open-source world.</p> <p>Today, all the major players in the container ecosystem adhere to the OCI container specification. For those interested in understanding how containers actually work, it is an invaluable technical resource that should not be overlooked.</p>"},{"location":"ch1/#overview","title":"Overview","text":"<p>OCI has two specs, an Image spec and a Runtime spec.</p> <p>And, if a diagram suits your taste better, here is what they cover and how they interact.</p> <pre><code>      Image Spec              Runtime Spec\n                 |\nconfig           | runtime config\nlayers           | rootfs\n|                |     |               delete\n|                |     |                  |\n|         unpack |     |     create       |     start/stop/exec\nImage (spec) ----|-&gt; Bundle ---------&gt; container  -------&gt;  process\n                 |\n                 |\n</code></pre> <p>An OCI Image will be downloaded from somewhere (thinking docker hub) and then it will be unpacked into an OCI Runtime filesystem bundle. From that point, the OCI Runtime Bundle will be run by an OCI Runtime. The Runtime Specification defines how to run a \"filesystem bundle\".</p>"},{"location":"ch1/#image-spec","title":"Image Spec","text":"<p>The image specification defines the archive format of OCI container images, which consists of a manifest, an image index, a set of filesystem layers, and a configuration. The goal of this specification is to enable the creation of interoperable tools for building, transporting, and preparing a container image to run.</p> <p>At the top level, a container image is just a tarball, and after being extracted, it has the <code>layout</code> as below.</p> <pre><code>\u251c\u2500\u2500 blobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sha256\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 4297f0*      (image.manifest)\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 7ea049       (image.config)\n\u251c\u2500\u2500 index.json\n\u2514\u2500\u2500 oci-layout\n\n* take only the first 6 digits for clarity\n</code></pre> <p>The layout isn't that useful without a specification of what that stuff is and how they are related (referenced).</p> <p>We can ignore the file <code>oci-layout</code> for simplicity. <code>index.json</code> is the entry point, it contains primary a <code>manifest</code>, which listed all the \"resources\" used by a single container image.</p> <p>The <code>manifest</code> contains primarily the <code>config</code> and the <code>layers</code>.</p> <p>Put that into a diagram, roughly this:</p> <pre><code>index.json -&gt; manifest -&gt;Config\n                |           | ref\n                |           |\n                |--------Layers --&gt; [ Base, upperlayer1,2,3...]\n</code></pre> <p>The config contains notably 1) configurations of the image, which can and will be converted to the runtime configure file of the runtime bundle, and 2) the layers, which makes up the root file system of the runtime bundle, and 3) some meta-data regarding the image history.</p> <p>layers are what makes up the final <code>rootfs</code>. The first layer is the base, all the other layers contain only the changes to its base. And this probably deserves more explanation.</p>"},{"location":"ch1/#layers","title":"Layers","text":"<p>For layers, the specification essentially defines two things:</p> <ol> <li> <p>How to represent a layer?</p> </li> <li> <p>For the base layer, <code>tar</code> all the content;</p> </li> <li>For non base layers, <code>tar</code> the changeset compared with its base.</li> </ol> <p>Hence, first detect the change, form a <code>changeset</code>; and then tar the changeset, as the representation of this layer.</p> <ol> <li>How to union all the layers?</li> </ol> <p>Apply all the changesets on top of the base layer. This will give you the <code>rootfs</code>.</p>"},{"location":"ch1/#runtime-specification","title":"Runtime Specification","text":"<p>Once the Image is unpacked into a runtime bundle on the disk file system, it becomes something you can run. At this point, the Runtime Specification comes into play. The Runtime Specification outlines the configuration, execution environment, and lifecycle of a container.</p> <p>A container's configuration contains the metadata necessary to create and run a container. This includes the process to run, environment variables, resource constraints, sandboxing features, and more. Some of these configurations are generic and can be used across all platforms, including Linux, Windows, Solaris, and specific virtual machines. However, some configurations are platform-specific, such as those for Linux only.</p> <p>The Runtime Specification also defines the lifecycle of a container, which is a series of events that occur from the moment a container is created until it ceases to exist.</p>"},{"location":"ch1/#container-lifecycle","title":"Container Lifecycle","text":"<p>A container has a lifecycle, at its essence, as you can imagine, it can be model as following state diagram.</p> <p>You can throw in a few other actions and states, such as <code>pause</code> and <code>paused</code>, but those are the fundamental ones.</p> <pre><code>                               +--------+       +----------+\n                               +prestart|       |poststart |\n                               | hook   |       | hook     |\n                               +--------+       +----------+\n    create   +---------+   start   |  +---------+   |\n  +---------&gt;| created |           |  | started |   |\n             |         |-------------&gt;|         |----\n             +---------+              +----+----+\n                                           |\n                                           v  stop\n             +---------+              +---------+\n             | deleted |              | stopped |\n             |         |&lt;-------------|         |\n             +---------+  delete   |  +---------+\n                                   |\n                               +---------+\n                               |poststop |\n                               |  hook   |\n                               +---------+\n</code></pre> <p>The state diagram is conventional, but there is one important detail worth mentioning - the <code>Hooks</code>. Contrary to what you might expect, the container specification does not define how to set up the network. Instead, it relies on technical constructs known as hooks to properly set up the network. For example, a hook might create the network before the container starts and delete it after the container stops.</p>"},{"location":"ch1/#container-configurations","title":"Container Configurations","text":"<p>Earlier, we mentioned that a container's configuration contains the necessary settings to create and run a container. Now, we will examine some of these configurations more closely to better understand what a container really is. For the purpose of this discussion, we will focus on configurations specific to the Linux platform.</p> <p>Here is an example configuration file:</p> <pre><code>{\n  \"ociVersion\": \"1.0.1\",\n  \"process\": {\n    \"terminal\": true,\n    \"user\": {\n      \"uid\": 0,\n      \"gid\": 0\n    },\n    \"args\": [\n      \"sh\"\n    ],\n    \"env\": [\n      \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n      \"TERM=xterm\"\n    ],\n    \"cwd\": \"/\"\n  },\n  \"root\": {\n    \"path\": \"rootfs\",\n    \"readonly\": true\n  },\n  \"hostname\": \"container-example\",\n  \"linux\": {\n    \"namespaces\": [\n      {\n        \"type\": \"pid\"\n      },\n      {\n        \"type\": \"network\"\n      },\n      {\n        \"type\": \"ipc\"\n      },\n      {\n        \"type\": \"uts\"\n      },\n      {\n        \"type\": \"mount\"\n      }\n    ]\n  }\n}\n</code></pre> <ul> <li> <p>Root It defines the root file system of the container.</p> </li> <li> <p>Mounts It specifies addition filesystem you can mount into the root file system. This is the place you can either bind mount your local host dir or a distributed dir, such as Ceph.</p> </li> <li> <p>Process It specifies all the things related to the process that you want to run inside the container. It includes environment variable and the arguments to the process.</p> </li> </ul> <p>For the Linux process, you can additionally specify things concerning the security aspect of the process, things such as the capabilities, rlimits, and selinux label can be specified here.</p> <ul> <li> <p>Hooks This is the place you can hook up into the container lifecycle and do things such as setting up and/or clean up the network.</p> </li> <li> <p>Linux Namespaces</p> </li> </ul> <p>A significant portion of the configurations for the Linux platform is dedicated to Namespace configuration. In fact, namespaces are the foundation of container technology. Put another way, there would be no containers without namespaces. Linux provides seven types of namespaces, all of which are supported by the OCI runtime specification:</p> Namespace Domain / Description PID Process IDs Mount Mount points Network Network devices, stacks, ports, etc User User and group IDs IPC System V IPC, POSIX message queues UTS Hostname and NIS domain name <ul> <li>Annotations In addition to defining what and how the container should run, annotations allow you to label the containers. The ability to label and select containers based on certain properties is a fundamental requirement for a container orchestration platform.</li> </ul>"},{"location":"ch1/#image-container-and-processes","title":"Image, Container, and Processes","text":"<p><code>Containers</code> are created from (container) <code>Image</code>. You can create more than one containers from a single Image, and you can also repack the containers, usually with changes to the base image, to create a new Image.</p> <p>After you get the containers, you can run <code>process</code> inside of that container, without all the nice things about a container. Most notably, once we containerize an app, it is become self-contained and won't mess up with the host environment, and thus it should \"run everywhere (TM)\".</p> <p>Here is the relationship between the various concept, <code>Image</code>, <code>Container</code> and <code>Process</code> and it is vitally important to get them right.</p> <pre><code>        Images               Container       Processes\n                 +                      +\n                 |                      |\n           create|                      |\n+--------+       |  +---------+  start  |  +---------+\n|runtime +---------&gt;|  created|         |  | started |\n|Bundle  |       |  |         |-----------&gt;|         |\n|        |       |  +---------+         |  +----+----+\n+--------+       |                      |       |\n                 |                      |       v  stop\n                 |  +---------+         |  +---------+\n                 |  | deleted |         |  | stopped |\n                 |  |         |&lt;-----------|         |\n                 |  +---------+         |  +---------+\n                 |               delete |\n                 |                      |\n</code></pre>"},{"location":"ch1/#docker-and-kubernetes","title":"Docker and Kubernetes","text":"<p>Docker makes container an industry trend and probably there are lots of people considering docker is container and container is docker. Docker definitely deserves the credit here. But from the technical point of view, docker is the most widely used container implementation. The architecture of the docker implementation evolves very quick from version to version. As of the time of writing, it looks like below.</p> <pre><code>                       +---------------------+\n                       |                     |\n                       |  dockerInc/docker   |\n                       |                     |\n                       +--------+------------+\n                                |  use\n                                v\n                       +---------------------+\n                       |                     |\n                       |    moby/moby        |\n                       |                     |\n                       +--------+------------+\n                                |  use\n                                v\n+-------------------+  +---------------------+\n|                   |  |                     |\n| oci/runtime-spec  |  |containerd/containerd|\n|                   |  |                     |\n+---------+---------+  +--------+------------+\n          |                     |  use\n          |impl                 v\n          |            +---------------------+     +------------+\n          |            |                     |     |            |\n          +----------- |     oci/runc        |---&gt; |oci/runc/   |\n                       |                     |     |libcontainer|\n                       +---------------------+     +------------+\n</code></pre> <p>The diagram follows the format of <code>[github]Org/project</code>. Most of the components are originated from Docker, but they are currently under different github organization and project. At the top is the <code>docker</code> command tool we use daily, it is the commercial offering from Docker Inc; The <code>docker</code> tool relies on an open source project called moby, which in turn uses the <code>runC</code>, which is the reference implementation of the <code>oci runtime</code> specification. <code>runc</code> heavily depend on <code>libcontainer</code>, which was donated from Docker, Inc as well.</p>"},{"location":"ch1/#container-orchaestraion","title":"Container orchaestraion","text":"<p>If we only need to one or two containers, docker probably is all we need. But if we want to run dozens or thousands of containers we have more problems to solve. To name a few:</p> <ul> <li>scheduling: Which host to put a container?</li> <li>update: How to update the container image?</li> <li>scaling: How to add more containers when more processing capacity is needed?</li> </ul> <p>That is the job of <code>container orchestration</code> system. And Kubernetes is one of them, but as of now, I think there is no argument it is the most promising one. But we'll not deep dive into Kubernetes here, but will touch briefly from the perspective that how the container runtime fit into the container orchestration platform.</p> <p>Following diagram illustrate how the Kubernetes interact with the container runtime.</p> <pre><code>\n      +----------------+---------------------------------+\n      |                |---------------+                 |\n      |   k8s/CRI      |               |                 |\n      +----------------+        impl   |          impl   |\n                                 +-----+--------+  +-----+--+\n                                 |cri-containerd|  |cri-o   |\n                      +----------|              |  |        |\n                      |          +--------------+  +-----+--+\n                      |                           k8s    |\n+--------------+   +--v-----------+          container   |\n|    oci/      |   |  containerd/ |                      |\n| runtime-spec |   |  containerd  |                      |\n|              |   |              |                      |\n+----+---------+   +--+-----------+                      |\n     ^                |                              use |\n     |                |    use    +----------------------+\n     |impl            v           |\n     |             +-------------++       +-------------+\n     |             |              |       |             |\n     +-------------|  oci/runc    +-----&gt; |oic/runc/    |\n                   |              |       |libcontainer |\n                   +--------------+       +-------------+\n</code></pre> <p>Kubernetes decouple the runtime implementation using Container Runtime Interface. Simply speaking, CRI defines the interface to create, start, stop and delete a container. It allows pluggable container runtime for Kubernetes and you don't have to lock into one particular runtime. There are currently several implementations, such as <code>cri-containerd</code> and <code>cri-o</code>, both of which eventually will use <code>oci/runc</code>.</p>"},{"location":"ch1/#summary","title":"Summary","text":"<p>This is an overview of OCI container image and runtime specification. It covers the responsibility of each specification and how they cooperate with each other. We go over the container lifecycle and primary configurations for the runtime spec. And we then introduce the relationship between docker and runc, and finish the article with a brief introduction to container orchestration and how the container runtime fit into it. It's quite a lot stuff! However, to really understand what real container is, we need to go even deeper.</p>"},{"location":"ch2/","title":"Namespaces","text":"<p>As we mentioned earlier, without namespaces, there would be no containers.</p> <p>This article will not provide a description or overview of namespaces in Linux. You can find that information here and here.</p> <p>Instead, we'll delve into the practical aspects and see what exactly happens to the namespaces when we use common container commands. This will help you appreciate the role namespaces play in container technology.</p> <p>There are several types of namespaces, such as PID namespaces and mount namespaces. In this article, we'll focus on the changes in PID namespaces, but other namespaces follow similar rules. We'll use runc as the container runtime because it's simple, has a specification, is easy to modify for experimentation, and I can point you to the code when necessary. As we pointed out in the previous chapter, Docker uses runc as the container runtime, and the Docker command is quite similar to the runc command. So, Docker users should feel at home when we use the runc command here.</p> <p>If you'd like, follow the instructions here to install runc and prepare a busybox runtime bundle (or container).</p> <p>Let's get started.</p>"},{"location":"ch2/#run-container","title":"Run container","text":"<pre><code>sudo runc run xyxy12\n</code></pre> <p>To detect the newly created processes, we can use tool <code>execsnoop</code>.</p> <pre><code>$ git clone https://github.com/brendangregg/perf-tools.git\n$ cd perf-tools\n$ sudo ./execsnoop\n</code></pre> <p>After running the xyxy12, we should see something as below. The first column is the PID of the newly created process, which is 10123 and the process is <code>sh</code>.</p> <pre><code> 10100  10099 runc start xyxy12\n 10113  10052 sudo runc run xyxy12\n 10114  10113 runc run xyxy12\n 10120  10117 runc init\n 10123  10121 sh\n</code></pre> <p>To get the PID namespaces, we can customize the output format of <code>ps</code> command, as shown below, and the PIDNS is what we want.</p> <pre><code>$ sudo ps -p 10123 -o pid,pidns\n  PID      PIDNS\n 10123 4026532572\n</code></pre> <p>We can also use <code>/proc</code> filesystem to find out the namespaces information:</p> <pre><code>$ sudo ls -l /proc/10123/ns\ntotal 0\nlrwxrwxrwx 1 root root 0 Apr 26 17:03 cgroup -&gt; cgroup:[4026531835]\nlrwxrwxrwx 1 root root 0 Apr 26 16:33 ipc -&gt; ipc:[4026532571]\nlrwxrwxrwx 1 root root 0 Apr 26 16:33 mnt -&gt; mnt:[4026532569]\nlrwxrwxrwx 1 root root 0 Apr 26 16:33 net -&gt; net:[4026532574]\nlrwxrwxrwx 1 root root 0 Apr 26 16:33 pid -&gt; pid:[4026532572]\nlrwxrwxrwx 1 root root 0 Apr 26 16:33 user -&gt; user:[4026531837]\nlrwxrwxrwx 1 root root 0 Apr 26 16:33 uts -&gt; uts:[4026532570]\n</code></pre> <p>We have a few files here, each one representing a type of namespace. 'PID' represents the PID namespaces, while 'mnt' represents the mount namespaces. These files are symlinks pointing to the \"real\" namespaces to which the process belongs. Think of them as pointers pointing to some namespace object, denoted by an <code>inode</code> number, which is unique in the host system. If the namespace symlinks of two different processes point to the same <code>inode</code>, they belong to the same namespace. By default, if no new namespaces are created, they all belong to the same \"root\" or \"default\" namespace.</p> <p>You can also find out the namespaces of <code>sh</code> inside the container, but you need to use the PID in the container namespace, which is <code>1</code> for the <code>10123</code>. It's the same process but has a different PID in a different namespace. That's what PID namespaces are all about. Note that it is mandatory for the <code>/proc</code> to be set up properly during container creation.</p> <pre><code># inside the container\n# ls -l /proc/1/ns\ntotal 0\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 cgroup -&gt; cgroup:[4026531835]\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 ipc -&gt; ipc:[4026532571]\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 mnt -&gt; mnt:[4026532569]\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 net -&gt; net:[4026532574]\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 pid -&gt; pid:[4026532572]\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 user -&gt; user:[4026531837]\nlrwxrwxrwx    1 root     root    0 Apr 26 06:34 uts -&gt; uts:[4026532570]\n</code></pre> <p>Next, we want to check what processes are in the newly created PID namespace. Unfortunately, there isn't a direct way to find this information. Instead, we need to go over all the <code>/proc/&lt;pid&gt;/ns</code> files and aggregate all the PIDs that belong to the same namespace. Luckily, the tool cinf does exactly that.</p> <pre><code>$ sudo cinf -namespace 4026532572\nPID   PPID  NAME CMD  CGROUPS\n10123 10114 sh   sh  14:name=dsystemd:/\n                     13:name=systemd:/user/1000.user/c2.session/xyxy12\n                     12:pids:/xyxy12\n                     11:hugetlb:/user/1000.user/c2.session/xyxy12\n                     10:net_prio:/user/1000.user/c2.session/xyxy12\n                     9:perf_event:/user/1000.user/c2.session/xyxy12\n                     8:net_cls:/user/1000.user/c2.session/xyxy12\n                     7:freezer:/user/1000.user/c2.session/xyxy12\n                     6:devices:/user/1000.user/c2.session/xyxy12\n                     5:memory:/user/1000.user/c2.session/xyxy12\n                     4:blkio:/user/1000.user/c2.session/xyxy12\n                     3:cpuacct:/user/1000.user/c2.session/xyxy12\n                     2:cpu:/user/1000.user/c2.session/xyxy12\n                     1:cpuset:/xyxy12\n</code></pre> <p>At the moment, there is only one process, which is the \"init\" program of the container we started, the <code>sh</code> program. Ignore the <code>cgroup</code> for now; we'll discuss it in a later chapter.</p> <p>We can see that when a new container is created, a bunch of new namespaces are also created, and the \"init\" process of the container is placed into these namespaces. Effectively, the process is running in a container, and this means different things for different namespaces. For PID namespaces, it means that all the processes running in the container can only see the processes in the same process namespace, \"pid:[4026532572]\", or equivalently \"pid:xyxy12\". The <code>sh</code> process is considered as PID 1 inside the container, but it is 10123 on the host, and that's the role of PID namespaces. As you can see, we can actually use the terms 'container' and 'namespaces' interchangeably in this context.</p> <p>We are clear, hopefully, about what does <code>docker/runc run</code> do regarding the namespaces. How about <code>docker/runc exec</code>?</p>"},{"location":"ch2/#run-new-process-inside-a-container","title":"Run new process inside a container","text":"<p>Do this:</p> <pre><code>sudo runc exec xyxy12 /bin/top -b\n</code></pre> <p>From <code>execsnoop</code>, we can see the pids - in the runtime namespaces.</p> <pre><code> 10702  10701 runc exec xyxy12 /bin/top -b\n 10708  10704 runc init\n 10710  10709 /bin/top -b\n</code></pre> <p>We can use <code>runc ps</code>, which will the processes running in a container, and the PIDs listed are in the runtime namespaces, which is what we want. (One interesting difference is <code>execsnoop</code> say the parent of 10710 is 10709, but <code>runc ps</code> says it is 10702, which is the runc exec command, seems makes more sense.)</p> <pre><code>$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     10123 10114  0 16:29 pts/0    00:00:00 sh\nroot     10710 10702  0 16:38 pts/1    00:00:00 /bin/top -b\n</code></pre> <p>Unfortunately, the <code>runc ps</code> does not fully support the <code>-o pid,pidns</code> option. So we'll again use the <code>cinf</code> to find out the namespaces of the new running process (<code>top -b</code>)</p> <pre><code>$ sudo cinf -pid 10710\n\n NAMESPACE   TYPE\n\n 4026532569  mnt\n 4026532570  uts\n 4026532571  ipc\n 4026532572  pid\n 4026532574  net\n 4026531837  user\n\n</code></pre> <p>We can see that no new namespaces were created. The <code>/bin/top -b</code> command joined the namespaces of \"init\" process - the first process we run in the container.</p> <p>Let's list again the processes inside of the PID namespaces <code>4026532572</code>. Now, there are two: 10123 and 10710.</p> <pre><code>$ sudo cinf -namespace 4026532572\nPID   PPID  NAME  CMD  CGROUPS\n10123 10114 sh    sh   14:name=dsystemd:/\n                       13:name=systemd:/user/1000.user/c2.session/xyxy12\n                       12:pids:/xyxy12\n                       11:hugetlb:/user/1000.user/c2.session/xyxy12\n                       10:net_prio:/user/1000.user/c2.session/xyxy12\n                       9:perf_event:/user/1000.user/c2.session/xyxy12\n                       8:net_cls:/user/1000.user/c2.session/xyxy12\n                       7:freezer:/user/1000.user/c2.session/xyxy12\n                       6:devices:/user/1000.user/c2.session/xyxy12\n                       5:memory:/user/1000.user/c2.session/xyxy12\n                       4:blkio:/user/1000.user/c2.session/xyxy12\n                       3:cpuacct:/user/1000.user/c2.session/xyxy12\n                       2:cpu:/user/1000.user/c2.session/xyxy12\n                       1:cpuset:/xyxy12\n10710 10702 top /bin/top -b 14:name=dsystemd:/\n                         13:name=systemd:/user/1000.user/c2.session/xyxy12\n                         12:pids:/xyxy12\n                         11:hugetlb:/user/1000.user/c2.session/xyxy12\n                         10:net_prio:/user/1000.user/c2.session/xyxy12\n                         9:perf_event:/user/1000.user/c2.session/xyxy12\n                         8:net_cls:/user/1000.user/c2.session/xyxy12\n                         7:freezer:/user/1000.user/c2.session/xyxy12\n                         6:devices:/user/1000.user/c2.session/xyxy12\n                         5:memory:/user/1000.user/c2.session/xyxy12\n                         4:blkio:/user/1000.user/c2.session/xyxy12\n                         3:cpuacct:/user/1000.user/c2.session/xyxy12\n                         2:cpu:/user/1000.user/c2.session/xyxy12\n                         1:cpuset:/xyxy12\n</code></pre> <p>If we run <code>ps</code> inside of the container, we'll also see those processes (plus the <code>ps</code> itself). Their Pids are 1 and 9, instead of 10123 and 10710. Again, its PID namespaces in play here.</p> <pre><code>/ # ps -ef\nPID   USER     TIME  COMMAND\n    1 root      0:00 sh\n    9 root      0:00 /bin/top -b\n   18 root      0:00 ps -ef\n\n</code></pre> <p>Now we know that <code>docker/runc exec</code> actually starts the new process inside of the namespaces the container already created.</p>"},{"location":"ch2/#summary","title":"Summary","text":"<p>When running a container, new namespaces are created and the <code>init</code> process is started within these namespaces. When running a new process in a container, it will join the namespaces that were created when the container was initiated.</p> <p>In the \"normal\" case, the container creates its own namespaces. However, you can also specify a path for the container or processes to run in, instead of letting the container create its own namespaces.</p> <p>Now you understand exactly how PID namespaces are used in a container. If you can take an extra step to figure out what the mount namespaces are and how they are used in a container, then you will understand the core of application containerization.</p>"},{"location":"ch3/","title":"CGroups","text":"<p>While namespaces control what the processes inside the containers can see, cgroups control how much resources they can use. Namespaces are about isolation, while cgroups are about resource \"budget\" control.</p> <p>Following the same methodology as we did when walking through the usage of namespaces in a container, we'll first create and run a container. Then, we'll run a new process in the container and observe how the cgroup in the host system changes. After that, we'll explore how to configure the cgroup, first by manually manipulating the host cgroup files, and then using the runc config files and Docker CLI commands.</p>"},{"location":"ch3/#create-cgroup","title":"Create cgroup","text":"<p>Before creating a container, take a snapshot of the cgroups using the following command. <code>lscgroup</code> is a command-line tool that lists all the cgroups currently in the system, simply by walking through <code>/sys/fs/cgroup/</code>.</p> <pre><code>lscgroup | tee cgroup.b\n</code></pre> <p>Start a container and record the cgroups after that,</p> <pre><code>sudo runc run xyxy12\nlscgroup | tee cgroup.a\n</code></pre> <p>Check the cgroup difference:</p> <pre><code># the diff out is cleaned up a little bit\n$ diff cgroup.b cgroup.a\n&gt; cpuset:/xyxy12\n&gt; cpu:/user/1000.user/c2.session/xyxy12\n&gt; cpuacct:/user/1000.user/c2.session/xyxy12\n&gt; blkio:/user/1000.user/c2.session/xyxy12\n&gt; memory:/user/1000.user/c2.session/xyxy12\n&gt; devices:/user/1000.user/c2.session/xyxy12\n&gt; freezer:/user/1000.user/c2.session/xyxy12\n&gt; net_cls:/user/1000.user/c2.session/xyxy12\n&gt; perf_event:/user/1000.user/c2.session/xyxy12\n&gt; net_prio:/user/1000.user/c2.session/xyxy12\n&gt; hugetlb:/user/1000.user/c2.session/xyxy12\n&gt; pids:/xyxy12\n</code></pre> <p>As we can see, for each cgroup type, a new cgroup <code>xyxy12</code> is created under its parent cgroup. The parent cgroup is the cgroup of the <code>bash</code> session in which we issued the <code>runc run</code> command.</p>"},{"location":"ch3/#who-is-under-control","title":"Who is under control?","text":"<p>cgroup control processes, it mandates how much memory/cpu/etc. a process or a group of processes can use. Adding a process into cgroup, is simpling add the process pid to the groups's <code>task</code> list.</p> <p>First, find out the PID of the container('s init process).</p> <pre><code>$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     23472 23463  0 12:33 pts/0    00:00:00 /bin/sh\n</code></pre> <p>Check who are in the variouse cgroups (using memory and cpu cgroup as examples):</p> <pre><code>$ cat /sys/fs/cgroup/memory/user/1000.user/c2.session/xyxy12/tasks\n23472\n$ cat /sys/fs/cgroup/cpu/user/1000.user/c2.session/xyxy12/tasks\n23472\n</code></pre> <p>Okay, it's clear that the container's <code>init</code> process is placed into the newly created cgroups dedicated to that container. To complete the description, you can find out which cgroups a process is in by using the command <code>cat /proc/&lt;pid&gt;/cgroup</code>.</p> <p>We're quite satisfied with what we've discovered, but we still want to understand how new processes started in the container relate to the container itself.</p>"},{"location":"ch3/#join-cgroup","title":"Join cgroup","text":"<p>Start a new process inside the container <code>xyxy12</code>.</p> <pre><code>sudo runc exec xyxy12 /bin/top -b\n</code></pre> <p>Check if any new cgroups are created</p> <pre><code>$ lscgroup | tee cgroup.c\n$ diff cgroup.c cgroup.a\n</code></pre> <p>Nope. No new cgroups are created when exec a new process inside of an already running container.</p> <p>Then, how the newly created process is related to the cgroup created by the container? Find the process of the new process first. It is 32123, and note that it is in the runtime namespace.</p> <pre><code>$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     23472 23463  0 12:33 pts/0    00:00:00 /bin/sh\nroot     32123 32115  0 12:48 pts/1    00:00:00 /bin/top -b\n</code></pre> <p>Check the memory and cpu cgrups for the container. Fwiw, the pattern is  \"/sys/fs/cgroup//user/1000.user/c2.session/\" <pre><code>$ cat /sys/fs/cgroup/memory/user/1000.user/c2.session/xyxy12/tasks\n23472\n32123\n\n$ cat /sys/fs/cgroup/cpu/user/1000.user/c2.session/xyxy12/tasks\n23472\n32123\n</code></pre> <p>Alright, this means that the new process will be added to the cgroups created during the first container run.</p> <p>In summary, when a container is created, a new cgroup will be created for each type of resource, and all the processes running in the container will be placed into these groups. Therefore, the resources that the processes in the container can use can be controlled through these cgroups.</p>"},{"location":"ch3/#config-cgroups","title":"Config cgroups","text":""},{"location":"ch3/#hard-way","title":"Hard way","text":"<p>We now understand when the cgroups are created, and how processes are assigned to each group. Finally, it's time to see how to actually use the cgroup to impose constraints on the processes. The memory constraint/cgroup is the easiest to understand, so we'll use that as an example.</p> <p>However, if you check the memory cgroup configuration for <code>xyxy12</code>, it isn't set at all. I'm not sure where 9223372036854771712 comes from, but it's certainly not a useful limitation.</p> <pre><code>cat /sys/fs/cgroup/memory/user/1000.user/c2.session/xyxy12/memory.limit_in_bytes\n9223372036854771712\n</code></pre> <p>To config a limit is as easy as writing a <code>sysfs</code> file.</p> <pre><code># requires root\n# echo \"100000000\" &gt; /sys/fs/cgroup/memory/user/1000.user/c2.session/xyxy12/memory.limit_in_bytes\n# echo \"0\" &gt; /sys/fs/cgroup/memory/user/1000.user/c2.session/xyxy12/memory.swappiness\n</code></pre> <p>With this setting in place, any processes in the container won't be able to use more memory than 100M, once it exceeds, it will be killed, or paused, depending on the <code>memory.oom_control</code> setting.</p>"},{"location":"ch3/#easy-way","title":"Easy way","text":"<p>You are not supposed to configure the cgroup in this way, although this is what happens under the hood.</p> <p>For <code>runc</code>, it can be easily set in the <code>config.json</code> file. By adding the following configuration snippet in the linux.resources section, you can limit all the processes launched in a container to a maximum of 100M memory.</p> <pre><code>\"memory\": {\n    \"limit\": 100000000,\n    \"reservation\": 200000\n}\n</code></pre> <p>Under the hood, <code>runc</code> will write the file for you. This can be demonstrated by changing the value from 100000000 to 100000, which will cause an error:</p> <pre><code>container_linux.go:348: starting container process caused \"process_linux.go:402:\ncontainer init caused \\\"process_linux.go:367: setting cgroup config for procHooks\nprocess caused \\\\\\\"failed to write 100000 to memory.limit_in_bytes: write /sys/fs/cgroup/memory/user/1000.user/c2.session/xyxy12/memory.limit_in_bytes:\ndevice or resource busy\\\\\\\"\\\"\"\n</code></pre> <p>If you use Docker, the memory limit can be specified in the <code>docker run</code> command using the <code>--memory</code> option. This option will be converted into a configuration file that is passed to <code>runc</code>, which will then write the corresponding sysfs file.</p>"},{"location":"ch3/#summary","title":"Summary","text":"<p>We walked through how Linux cgroups are used in containers.</p> <ul> <li>A new cgroup will be created for each new container.</li> <li>Executing a new process in a running container will join the newly created cgroups.</li> <li>Configuring the cgroup properly ensures that the processes in the container are controlled.</li> </ul>"},{"location":"ch4/","title":"Capabilities","text":"<p>Capabilities are used to break down the super privileges enjoyed by the root user into fine-grained permissions. This means that even as a root user, you are not able to do whatever you want unless you have been granted the corresponding capabilities.</p>"},{"location":"ch4/#prepare-rootfs","title":"Prepare rootfs","text":"<p>We'll need to install some additional tool (libcap) to explore the capabilities, so here some instruction of how to prepare such a rootfs.</p> <p>First, create a docker container with libcap installed,</p> <pre><code>sudo docker run -it alpine sh -c 'apk add -U libcap; capsh --print'\n</code></pre> <p>Use <code>docker ps -a</code> to find out the container ID of the one we just ran; it should be the latest one.</p> <p>Then, export the rootfs to create a <code>runc</code> runtime bundle.</p> <pre><code>mkdir rootfs\ndocker export $container_id | tar -C rootfs -xvf -\nrunc spec\n</code></pre>"},{"location":"ch4/#capability","title":"Capability","text":"<p>To understand what capabilities are: Using the default <code>config.json</code> generated from <code>runc spec</code>, you are not allowed to set the hostname, even as root.</p> <pre><code>$ sudo runc run xyxy67\n/ # id\nuid=0(root) gid=0(root)\n/ # hostname cool\nhostname: sethostname: Operation not permitted\n</code></pre> <p>That's because setting the hostname requires the <code>CAP_SYS_ADMIN</code> capability, even for the root user. We can add this capability by including <code>CAP_SYS_ADMIN</code> in the <code>bounding</code>, <code>permitted</code>, and <code>effective</code> lists of the capabilities attribute for the init process.</p> <p>Run another container with the new configuration, and now you will be allowed to set the hostname.</p> <pre><code>$ sudo runc run xyxy67\n/ # hostname\nrunc\n/ # hostname hello\n/ # hostname\nhello\n/ #\n</code></pre> <p>Run another command in the same container, and it will able to set hostname as well since it inherits the capability of the init process.</p> <pre><code>$ sudo runc exec -t xyxy67 /bin/sh\n[sudo] password for binchen:\n/ # hostname\nhello\n/ # hostname good\n/ # hostname\ngood\n</code></pre>"},{"location":"ch4/#get-capability","title":"Get capability","text":"<p>Get the PID of the two processes in the runtime PID namespace.</p> <pre><code>$ sudo runc ps xyxy67\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     26002 25993  0 11:42 pts/0    00:00:00 /bin/sh\nroot     26059 26051  0 11:43 pts/1    00:00:00 /bin/sh\n</code></pre> <p>Install <code>pscap</code> on the host:</p> <pre><code>sudo apt-get install libcap-ng-utils\n</code></pre> <p>Check capabilities of the running process using the pids in the host namespace.</p> <pre><code>$ pscap | grep \"26059\\|26002\"\n25993 26002 root        sh                kill, net_bind_service, sys_admin, audit_write\n26051 26059 root        sh                kill, net_bind_service, sys_admin, audit_write\n</code></pre> <p>And we can confirm those two process has the <code>sys_admin</code> capability.</p>"},{"location":"ch4/#request-additional-capability","title":"Request additional capability","text":"<p>The exec can require additional caps that don't exist in the <code>config.json</code>.</p> <p>Run another container <code>xyxy78</code> without the <code>CAP_SYS_ADMIN</code> in the <code>config.json</code>.</p> <p>Double check it indeed doesn't have the CAPS.</p> <pre><code>$ sudo runc ps xyxy78\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     27385 27376  0 11:57 pts/0    00:00:00 /bin/sh\n$ pscap | grep 27385\n27376 27385 root        sh                kill, net_bind_service, audit_write\n</code></pre> <p>Start another process in <code>xyxy78</code> but with additional CAP_SYS_ADMIN capability, using <code>--cap</code> option.</p> <pre><code>sudo runc exec --cap CAP_SYS_ADMIN xyxy78 /bin/hostname cool\n</code></pre> <p>Under the hood, the <code>--cap</code> option sets up the capability list for the process that will be executed, similar to how these settings are established in the <code>config.json</code> for the init process.</p>"},{"location":"ch4/#capsh","title":"capsh","text":"<p>You can use capsh explore a little bit more.</p> <p>Run <code>capsh --print</code> inside of the container.</p> <p>This is the output with default config.json:</p> <pre><code># capsh --print\nCurrent: = cap_kill,cap_net_bind_service,cap_audit_write+eip\nBounding set =cap_kill,cap_net_bind_service,cap_audit_write\nSecurebits: 00/0x0/1'b0\n secure-noroot: no (unlocked)\n secure-no-suid-fixup: no (unlocked)\n secure-keep-caps: no (unlocked)\nuid=0(root)\ngid=0(root)\ngroups=\n</code></pre> <p>This is the output with the added <code>CAP_SYS_ADMIN</code> capability. Compared with the previous one, we can see an additional <code>cap_sys_admin+ep</code> in the \"Current\" section and <code>cap_sys_admin</code> in the \"Bounding Set\". The \"+ep\" indicates that the preceding capabilities are in both the \"effective\" and \"permitted\" lists. For more information regarding the capability list, see capabilities.</p> <pre><code># capsh --print\nCurrent: = cap_kill,cap_net_bind_service,cap_audit_write+eip cap_sys_admin+ep\nBounding set =cap_kill,cap_net_bind_service,cap_sys_admin,cap_audit_write\nSecurebits: 00/0x0/1'b0\n secure-noroot: no (unlocked)\n secure-no-suid-fixup: no (unlocked)\n secure-keep-caps: no (unlocked)\nuid=0(root)\ngid=0(root)\ngroups=\n</code></pre>"},{"location":"ch4/#summary","title":"Summary","text":"<p>We investigated how Linux capability is used to limit the things a process can do and thus increase the security of the container.</p>"},{"location":"ch5/","title":"Mount namespace &amp; pivot_root","text":"<p>Files in a Linux system are organized as a tree. The tree normally starts with a root file system (called <code>rootfs</code>) provided by the Linux distribution, and the rootfs is mounted as \"/\". Later, additional file systems can optionally be attached to a subdirectory, such as /data, which could point to an disk.</p> <p><code>mount(2)</code> is the system call used to attach a file system or a directory to a node of the root tree. When the system boots up, the init process performs multiple mount calls to set up the file system properly, creating the initial mount table. All processes have their own mount table, but they normally point to the same one - the one set up by the init process. However, a process can also have a separate mount table from its parent. It starts with a copy of the parent's table, but any subsequent changes to it (incurred by <code>mount</code>) will only impact itself. This is what the <code>mount namespace</code> is for. It's worth noting that in the same mount namespace, any changes to the mount table by one process will be visible to another process. Because of this, when you mount a USB disk on the shell, the file explorer will be able to see the content as well.</p>"},{"location":"ch5/#mount-namespace","title":"Mount Namespace","text":"<p>Normally, an application won't create a separate mount namespace when it starts.</p> <p>For example, there are two mount namespaces on my host:</p> <pre><code>$ sudo cinf | grep mn\n 4026531857  mnt  1  0\n 4026531840  mnt  311 0,1,7,101,102,106,107,109,111,113,116,121,125,126,127,1000,65534  /sbin/init\n</code></pre> <p>But the first one has only one process kdevtmpfs, which is a kernel process.</p> <pre><code>$ sudo cinf --namespace 4026531857\n\n PID  PPID  NAME       CMD  NTHREADS  CGROUPS             STATE\n 46   2     kdevtmpfs       1         xxx  S (sleeping)\n\n$ ps -ef | grep kdevtmpfs\nroot        46     2  0 10:17 ?        00:00:00 [kdevtmpfs]\n</code></pre> <p>All the other processes are in the second mount namespace created by <code>/sbin/init</code>. If you check the mount points of two processes in that mount namespace (by using <code>cat /proc/pid/mounts</code>), they are all the same.</p>"},{"location":"ch5/#mnt-namespace-for-container","title":"Mnt Namespace for Container","text":"<p>Let's start a container and see what changes in the mount namespaces.</p> <pre><code>sudo runc run xyxy12\n</code></pre> <p>Check the mount namespace</p> <pre><code>$ sudo cinf | grep mnt\n 4026531840  mnt 333 0,1,7,101,102,106,107,109,111,113,116,121,125,126,127,1000,65534  /sbin/init\n 4026532458  mnt 1   0         sh\n 4026531857  mnt 1   0\n</code></pre> <p>We have a new mount namespace, 4026532458, which is created when run container xyxy12:</p> <pre><code>$ sudo cinf -namespace 4026532458\n PID    PPID   NAME  CMD  NTHREADS  CGROUPS                                            STATE\n 11674  11665  sh    sh   1         14:name=dsystemd:/                                 S (sleeping)\n\n$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     11674 11665  0 12:00 pts/0    00:00:00 sh\n</code></pre> <p>And here is the dump of the mount info for our new container.</p> <pre><code>$ cat /proc/11674/mounts | sort | uniq\ncgroup /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\ncgroup /sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct 0 0\ncgroup /sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu 0 0\ncgroup /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\ncgroup /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\ncgroup /sys/fs/cgroup/dsystemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=dsystemd 0 0\ncgroup /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\ncgroup /sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\ncgroup /sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\ncgroup /sys/fs/cgroup/net_cls cgroup ro,nosuid,nodev,noexec,relatime,net_cls 0 0\ncgroup /sys/fs/cgroup/net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_prio 0 0\ncgroup /sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\ncgroup /sys/fs/cgroup/pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0\n/dev/disk/by-uuid/22cb3888-325e-4283-a605-d2f60d11bb96 / ext4 ro,relatime,errors=remount-ro,data=ordered 0 0\n/home/binchen/container/runc/devpts /dev/console devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0\n/home/binchen/container/runc/devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0\n/home/binchen/container/runc/mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0\n/home/binchen/container/runc/proc /proc/asound proc ro,relatime 0 0\n/home/binchen/container/runc/proc /proc/bus proc ro,relatime 0 0\n/home/binchen/container/runc/proc /proc/fs proc ro,relatime 0 0\n/home/binchen/container/runc/proc /proc/irq proc ro,relatime 0 0\n/home/binchen/container/runc/proc /proc proc rw,relatime 0 0\n/home/binchen/container/runc/proc /proc/sys proc ro,relatime 0 0\n/home/binchen/container/runc/proc /proc/sysrq-trigger proc ro,relatime 0 0\n/home/binchen/container/runc/shm /dev/shm tmpfs rw,nosuid,nodev,noexec,relatime,size=65536k 0 0\n/home/binchen/container/runc/sysfs /sys sysfs ro,nosuid,nodev,noexec,relatime 0 0\n/home/binchen/container/runc/tmpfs /dev tmpfs rw,nosuid,size=65536k,mode=755 0 0\n/home/binchen/container/runc/tmpfs /proc/kcore tmpfs rw,nosuid,size=65536k,mode=755 0 0\n/home/binchen/container/runc/tmpfs /proc/sched_debug tmpfs rw,nosuid,size=65536k,mode=755 0 0\n/home/binchen/container/runc/tmpfs /proc/timer_list tmpfs rw,nosuid,size=65536k,mode=755 0 0\n/home/binchen/container/runc/tmpfs /proc/timer_stats tmpfs rw,nosuid,size=65536k,mode=755 0 0\nname=systemd /sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,name=systemd 0 0\ntmpfs /proc/scsi tmpfs ro,relatime 0 0\ntmpfs /sys/firmware tmpfs ro,relatime 0 0\ntmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,relatime,mode=755 0 0\n</code></pre> <p>The content might not be of much interest to you. We will skip the details of most of these entries, except for one:</p> <pre><code>/dev/disk/by-uuid/22cb3888-325e-4283-a605-d2f60d11bb96 / ext4 ro,relatime,errors=remount-ro,data=ordered 0 0\n</code></pre> <p>This mount src is pointing to the <code>/dev/sda2</code>, which is our host's rootfs mounting to.</p> <pre><code>$ ll /dev/disk/by-uuid/22cb3888-325e-4283-a605-d2f60d11bb96\nlrwxrwxrwx 1 root root 10 Apr 28 13:28 /dev/disk/by-uuid/22cb3888-325e-4283-a605-d2f60d11bb96 -&gt; ../../sda2\n\n$ mount\n/dev/sda2 on / type ext4 (rw,errors=remount-ro)\n</code></pre> <p>Does that sound surprising and alarming to you? Why is the root of the container is same as the root of the host? So with a new mount namespace, we still can access the root of the host? Shouldn't the container be \"jailed\" in the rootfs the contained is started in?</p> <p>Let's check one more thing, compare the inode number of the <code>/</code> in the container and the inode of the container's rootfs.</p> <pre><code># in container\n/ # ls -di /\n25846165 /\n</code></pre> <pre><code># on host\n$ ls -di ~/container/runc/rootfs/\n25846165 /home/binchen/container/runc/rootfs/\n</code></pre> <p>They are same! That means the root of the container is the rootfs, or directory of its runtime bundle, in OCI's term, as you expected!</p> <p>So, why? From the mount we see \"/\" is mounted to /dev/sda2, which is same as the root of the host but in fact, the root is the container bundle directory?</p> <p>Entering <code>pivot_root</code>.</p>"},{"location":"ch5/#pivot_root","title":"pivot_root","text":"<p>The \"jail\" is done by pivot_root, which changes the root of the process to the runtime bundle directory.</p> <p>This is the code did that magic. The latest version looks less easy to understand than the earlier version since it used an idea from lxc making privot_root working on read-only rootfs, so there is no need to create a temporary writable directory.</p>"},{"location":"ch5/#chroot","title":"chroot","text":"<p>It won't be complete if we wouldn't mention <code>chroot(2)</code> when talking about the filesystem for the container. However, it is not mandatory to create a new mnt namespace and use privot_root. Optionally, but less ideally, you can use <code>chroot(2)</code>, which will \"jail\" the calling process (and all its children) into the rootfs the container starts with. </p> <p>Unlike the mount namespace, <code>chroot</code> won't change anything to mount, it just changes the process path lookup, interpreting the <code>/</code> as the path chroot-ed to; for the difference between <code>chroot</code> and <code>privot_root</code> see here. In short, privot_root is more thorough, and safer.</p> <p>To use chroot, if you like, make following changes to your default config.json. In addition to removal of  <code>type:mount</code>, we have removed \"maskedPaths\" and \"readonlyPaths\" which will require a private mount namespace to work.</p> <pre><code>diff --git a/config.json b/config.json\nindex 25a3154..9382207 100644\n--- a/config.json\n+++ b/config.json\n@@ -152,27 +152,7 @@\n                        },\n                        {\n                                \"type\": \"uts\"\n-                       },\n-                       {\n-                               \"type\": \"mount\"\n                        }\n-               ],\n-               \"maskedPaths\": [\n-                       \"/proc/kcore\",\n-                       \"/proc/latency_stats\",\n-                       \"/proc/timer_list\",\n-                       \"/proc/timer_stats\",\n-                       \"/proc/sched_debug\",\n-                       \"/sys/firmware\",\n-                       \"/proc/scsi\"\n-               ],\n-               \"readonlyPaths\": [\n-                       \"/proc/asound\",\n-                       \"/proc/bus\",\n-                       \"/proc/fs\",\n-                       \"/proc/irq\",\n-                       \"/proc/sys\",\n-                       \"/proc/sysrq-trigger\"\n                ]\n        }\n+}\n</code></pre> <p>If we re-do the exercise we did previously, you will find no new namespace will be created this time, but you can't list the files outside of the rootfs.</p> <p>Throw some code here to make things more clear. Ignore NoPivotRoot at the moment and assume it is already false.</p> <pre><code>if config.NoPivotRoot {\n        err = msMoveRoot(config.Rootfs)\n} else if config.Namespaces.Contains(configs.NEWNS) {\n    err = pivotRoot(config.Rootfs)\n} else {\n    err = chroot(config.Rootfs)\n}\n</code></pre>"},{"location":"ch5/#bind-mount","title":"Bind Mount","text":"<p><code>bind mount</code> is a feature supported by Linux that allows a portion of the file hierarchy to be remounted elsewhere, making it accessible from both locations. This is often used to share a host directory with a container.</p> <p>The following change to the <code>config.json</code> instructs the container runtime to bind mount a local directory (<code>host_dir</code>, a relative path to the runtime bundle) to a directory (<code>/host_dir</code>, an absolute path in the container's root file system).</p> <pre><code>diff --git a/config.json b/config.json\nindex 25a3154..13ae9bf 100644\n--- a/config.json\n+++ b/config.json\n@@ -129,6 +129,11 @@\n                                \"relatime\",\n                                \"ro\"\n                        ]\n+               },\n+               {\n+                       \"destination\": \"/host_dir\",\n+                       \"type\": \"bind\",\n+                       \"source\": \"host\"\n+                       \"options\" : [\"bind\"]\n</code></pre> <p>For bind mount to work, the host directory must exist before mounting, but not the bind destination dir, which will be created by the container runtime if not exists. Here is (part) the directory tree:</p> <pre><code>:~/container/runc$ tree -L 2\n.\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 host_dir  &lt;- host dir will be bind mounted\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 hi\n\u251c\u2500\u2500 rootfs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bin\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 etc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 home\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host\n\n</code></pre> <p>Start the container with the new config.json, and we can see the content of <code>host_dir</code> through <code>/host</code> in the container.</p> <pre><code>/ # ls /host/\nhi\n</code></pre> <p>However, since the bind mount is happening in the mount namespace for the container, not on the host, you won't be able to see anything in the <code>rootfs/host</code>.</p> <p>We can double check this by looking at the mount info inside of the container:</p> <pre><code># inside of the container\n# cat /proc/self/mountinfo | grep host_dir\n212 166 8:2 /home/binchen/container/runc/host_dir /host rw,relatime - ext4 /dev/disk/by-uuid/22cb3888-325e-4283-a605-d2f60d11bb96 rw,errors=remount-ro,data=ordered\n</code></pre>"},{"location":"ch5/#exercise-access-host-usb","title":"Exercise: Access Host USB","text":"<p>Let's do more exercises on how to access a host USB disk. why? Because USB disk is a volume device, it aligns with our topic today regarding data or filesystem in a container! Besides, as we'll see later, bind mount can be used not only for mounting a host directory, but also a host device file, into the container.</p> <p>Make the following changes to the default <code>config.json</code> and we'll explain it shortly.</p> <pre><code>diff --git a/config.json b/config.json\nindex 25a3154..5e58226 100644\n--- a/config.json\n+++ b/config.json\n@@ -3,8 +3,8 @@\n        \"process\": {\n                \"terminal\": true,\n                \"user\": {\n-                       \"uid\": 0,\n-                       \"gid\": 0                              (3)\n+                       \"uid\": 1000,\n+                       \"gid\": 1000\n                },\n                \"args\": [\n                        \"sh\"\n@@ -129,6 +129,16 @@\n                                \"relatime\",\n                                \"ro\"\n                        ]\n+               },{\n+                       \"destination\": \"/dev/usb\",\n+                       \"type\": \"bind\",                      (1)\n+                       \"source\": \"/dev/sdb1\"\n+                       \"options\": [\"bind\"]\n+               },\n+               {\n+                       \"destination\": \"/usb2\",\n+                       \"type\": \"vfat\",                      (2)\n+                       \"source\": \"/dev/sdb1\",\n+                       \"options\": [\"rw\"]\n                }\n        ],\n</code></pre> <p>start the container with the new config and we will be able to read and write to the usb from the /usb2 directory inside of the container.</p> <pre><code>/usb2 $ echo \"hello usb\" &gt; container\n/usb2 $ cat container\nhello usb\n</code></pre> <p>Explain the changes we made:</p> <ul> <li><code>(1)</code> bind mount the device node from the host (<code>/dev/sdb1</code>) to the container(<code>/dev/usb</code>).</li> <li><code>(2)</code> mount the device(<code>/dev/usb</code>) to a directory inside of the container (<code>/usb2</code>)</li> <li><code>(3)</code> set up the uid/guid the bash process to the same as the uid/guid of the usb2 directory. Without this change, we will hit permission issues. We'll have another article on the user and permission in container, for now, just set the uid:gid as we have done here.</li> </ul> <pre><code>#in container\n# cd usb2\nsh: cd: can't cd to usb2: Permission denied\n</code></pre>"},{"location":"ch5/#user-permission-change-after-mount","title":"User Permission Change After Mount","text":"<p>Before mount,</p> <pre><code> File: \u2018usb2\u2019\n  Size: 4096          Blocks: 8          IO Block: 4096   directory\nDevice: 802h/2050d    Inode: 25879017    Links: 2\nAccess: (0777/drwxrwxrwx)  Uid: ( 1000/ binchen)   Gid: ( 1000/ binchen)\nAccess: 2018-05-11 09:38:17.639411272 +1000\nModify: 2018-05-10 15:13:05.321299162 +1000\nChange: 2018-05-10 15:15:29.541298098 +1000\n Birth: -\n</code></pre> <p>Mount it:</p> <pre><code>sudo mount -t vfat /dev/sdb1 usb2\n</code></pre> <p>After it:</p> <pre><code>$ stat usb2\n  File: \u2018usb2\u2019\n  Size: 8192          Blocks: 16         IO Block: 8192   directory\nDevice: 811h/2065d    Inode: 1           Links: 3\nAccess: (0700/drwx------)  Uid: ( 1000/ binchen)   Gid: ( 1000/ binchen)\nAccess: 1970-01-01 10:00:00.000000000 +1000\nModify: 1970-01-01 10:00:00.000000000 +1000\nChange: 1970-01-01 10:00:00.000000000 +1000\n Birth: -\n</code></pre> <p>Notice the inode changes (from 25879017 to 1), so does the permissions (from 0777 to 0700).</p> <p>So after the mount, usb2 becomes the root file system of the sdb1 device, so the inode becomes 1, which is the first inode in a partition, and the new permission is the permission of that file system, not the permission of the mounting point!</p> <p>If you want to change the owner/permission after the device being mounted, you chown/chmod. Or, you can use the options for the mount. (But, it doesn't work for vfat as I tried.)</p> <p>see also <code>mount</code>.</p> <p>The previous contents (if any)  and owner and mode of dir become invisible, and as long as this filesystem remains mounted, the pathname dir refers to the root of the filesystem on the device.</p>"},{"location":"ch5/#docker-volume","title":"Docker Volume","text":"<p>Lastly, a few words on volume, which is a Docker terminology and is not covered by the OCI runtime spec. Fundamentally, it is still a mount, whether it's a bind mount of a directory (as we did in the host_dir mounting case) or a mount of a volume device (as we did in the USB case). We can think of volume as a \"managed mount service from Docker\" with a handy CLI interface.</p>"},{"location":"ch5/#summary","title":"Summary","text":"<p>We discussed how a container creates a new mount namespace and confines the processes inside the container's root file system. We also talked about how a container uses mount and bind mount to access and share the host device and directory. We skimmed over the concept of volume from Docker, which is fundamentally a \"managed mount\".</p>"},{"location":"ch6/","title":"Root and User namespaces","text":"<p><code>User</code> and <code>permission</code> are the oldest and most basic security mechanisms in Linux. Briefly, here's how it works: </p> <p>1) The system has a number of users and groups 2) Every file belongs to an owner and a group 3) Every process belongs to a user and one or more groups 4) Lastly, to link 1, 2, and 3 together, every file has a <code>mode</code> setting that defines the permissions for three types of processes: owner, group, and others</p> <p>Note that the kernel knows and cares only about UID and GID, not the user name and group name.</p>"},{"location":"ch6/#set-uid-of-container-processes","title":"Set uid of container processes","text":"<p>User property can be used to specify under which user the process will be run as. It is optional and by default, it is 0 or <code>root</code>, which is required to run the <code>runc</code>.</p> <p>That means you can delete following section from the default <code>config.json</code> and will still be able to start the container.</p> <pre><code>diff --git a/config.json b/config.json\n-               \"user\": {\n-                       \"uid\": 0,\n-                       \"gid\": 0\n-               },\n</code></pre> <p>Start the container and list the user.</p> <pre><code>$ sudo runc run xyxy12\n/ # id\nuid=0(root) gid=0(root)\n</code></pre> <p>On host,</p> <pre><code>binchen@m:~/container/runc$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot     27544 27535  0 12:05 pts/0    00:00:00 sh\n</code></pre> <p>As seen, it is running as <code>root</code>.</p> <p>Running a container process as root is worrisome. But fortunately, by default, the container process, even being run as root, has other extra constraints (such as capability) in place, so they are usually less powerful then the root on the host which usually by default has more capability assigned.</p> <p>But still, it is more secure to run the process as a non-privileged normal user, and you can do so by specifying the uid/guid as non-zero.</p> <p>Let's change the uid/guid of the user config to 1000 and start the container.</p> <pre><code>/ $ id\nuid=1000 gid=1000\n</code></pre> <p>It doesn't mention the username since there isn't one in the container, but from the host side (see the UID):</p> <pre><code>binchen@m:~/container/runc$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nbinchen  24904 24895  0 11:44 pts/0    00:00:00 sh\n</code></pre> <p>By default, create a container won't create a new user namespace and the uid you see in the container and on the host are the same user - i.e share the same user namespace, to say it in a fancy way.</p>"},{"location":"ch6/#user-namespace-and-uidgid-mapping","title":"User Namespace and UID/GID mapping","text":"<p>Let's see what happens when using a user namespace.</p> <p>Here is the user namespace before starting container with namespace support:</p> <pre><code>$ sudo cinf | grep user\n 4026531837 user 297 0,1,7,101,102,106,107,109,111,113,116,121,125,126,127,1000,65534  /sbin/init\n 4026532254 user 1   1000                                                              /opt/google/chrome/n\n 4026532423 user 25  1000                                                              /opt/google/chrome/c\n</code></pre> <p>Making following changes to enable user namespace:</p> <pre><code>$ git diff\ndiff --git a/config.json b/config.json\nindex 25a3154..466eae8 100644\n--- a/config.json\n+++ b/config.json\n@@ -155,6 +155,23 @@\n                        },\n                        {\n                                \"type\": \"mount\"\n+                       },\n+                       {\n+                               \"type\": \"user\"\n+                       }\n+               ],\n+               \"uidMappings\": [\n+                       {\n+                               \"containerID\": 0,\n+                               \"hostID\": 1000,\n+                               \"size\": 32000\n+                       }\n+               ],\n+               \"gidMappings\": [\n+                       {\n+                               \"containerID\": 0,\n+                               \"hostID\": 1000,\n+                               \"size\": 32000\n                        }\n</code></pre> <p>It is an error to enable the user namespace without a UID/GID mapping. Similarly, UID/GID mapping is useless and will be ignored if the user namespace isn't enabled. This effectively results in an incorrect configuration.</p> <p>Start a container with the new configuration and list the user namespaces in the system:</p> <pre><code>$ sudo cinf | grep user\n 4026532423  user  25  1000                                                              /opt/google/chrome/c\n 4026532254  user  1   1000                                                              /opt/google/chrome/n\n 4026532450  user  1   1000    sh\n 4026531837  user  297 0,1,7,101,102,106,107,109,111,113,116,121,125,126,127,1000,65534  /sbin/init\n</code></pre> <p>We can see that we have a new user namespace (4026532450) and our new container process (sh) is running within it.</p> <p>Inside the container, it is running as UID/GID 0, which is considered to be root.</p> <pre><code>/ # id\nuid=0(root) gid=0(root)\n</code></pre> <p>However, from the outside, the process is indeed considered to be running as binchen, which is 1000.</p> <pre><code>binchen@m:~/container/runc$ sudo runc ps xyxy12\nUID        PID  PPID  C STIME TTY          TIME CMD\nbinchen   4356  4347  0 11:18 pts/0    00:00:00 sh\n</code></pre> <p>This is where the user namespace and UID/PID mapping come into play: UID 0 inside the container corresponds to UID 1000 on the host, a constant offset as specified in the mapping. You can view the offset or mapping on the host by checking the proc as follows:</p> <pre><code>binchen@m:~/container/runc$ cat /proc/4356/uid_map\n         0       1000      32000\n</code></pre>"},{"location":"ch6/#exercise","title":"Exercise","text":"<p>Let's do some exercises to verify that the '0' inside the container is actually '1000' on the host, and ultimately, it's '1000' that the kernel checks.</p> <p>Inside the rootfs but on the host, create two directories, <code>bindir</code> and <code>rootdir</code>. These should be owned by the current user (id:1000) and root, respectively, and should only be accessible by their respective owners.</p> <p>Type following commands:</p> <pre><code>mkdir bindir\nchmod 700 bindir\n\nmkdir rootdir\nsudo chgrp 0 rootdir\nsudo chown 0 rootdir\nsudo chmod 700 rootdir\n</code></pre> <p>Here is what it should look like:</p> <pre><code>drwx------   2 binchen binchen  4096 May 10 11:27 bindir/\ndrwx------   2 root    root     4096 May 10 11:27 rootdir/\n</code></pre> <p>On the host, test the group and permission, The exception is the current user (binchen) can enter into bindir but not rootdir. After you switch to the root, the root can access not only rootdir (since root owns that dir) but also bindir (because it is root!).</p> <p>To make the exercise more convincing, and let's change the uid/gid offset to 2000, so that the actual user maps to no-body on the host. And we'll expect inside of the container, the <code>root</code> can access none of the directories since the <code>root</code> in the container is uid 2000 and kernel won't allow it to access any of those directories.</p> <p>start the container:</p> <pre><code>binchen@m:~/container/runc$ sudo runc run xyxy12\n/ # id\nuid=0(root) gid=0(root)\n/ # ls -l\ndrwx------    2 nobody   nogroup       4096 May 10 01:27 bindir\ndrwx------    2 nobody   nogroup       4096 May 10 01:27 rootdir\n/ # cd bindir/\nsh: cd: can't cd to bindir/: Permission denied\n/ # cd ..\n/ # cd rootdir/\nsh: cd: can't cd to rootdir/: Permission denied\n</code></pre> <p>This is a great time to mention that you always have to make sure the rootfs (or runc runtime bundle) has the right permission setting that matches the user/gid mapping you want to use. The runtime won't modify the file system ownership to realize the mapping.</p>"},{"location":"ch6/#benefit","title":"Benefit","text":"<p>What are the benefits of using a user namespace?</p> <ol> <li> <p>A user namespace is useful when a process requires root access to run, but you don't want to grant it full root privileges. (Otherwise, simply using a non-zero user ID would suffice.)</p> </li> <li> <p>When there are multiple users (for different processes) within a single container, placing them in different user namespaces allows you to monitor and control multiple instances of the same container.</p> </li> </ol>"},{"location":"ch6/#summary","title":"Summary","text":"<p>Don't run your container process as root user; if you have to put it into a separate user namespace.</p>"},{"location":"ch7/","title":"Network","text":"<p>When it comes to container networking, the OCI runtime spec does nothing more than creating or joining a network namespace. All other tasks are left to be handled using hooks, which allow you to inject into different stages of the container runtime and perform some customization.</p> <p>With the default <code>config.json</code>, you will only see a <code>loop device</code>, not an <code>eth0</code> that you normally see on the host, which allows you to communicate with the outside world. However, we can set up a simple bridge network using <code>netns</code> as the hook.</p> <p>Download netns and copy the binary to <code>/usr/local/bin</code>, as assumed by the following <code>config.json</code>. It's worth noting that the hooks are executed in the runtime namespace, not the container namespace. This means, among other things, that the hooks binary should reside on the host system, not in the container. Therefore, you don't need to put <code>netns</code> into the container rootfs.</p>"},{"location":"ch7/#setup-bridge-network-using-netns","title":"Setup bridge network using netns","text":"<p>Make the following changes to <code>config.json</code>. In addition to the hooks, we also need the <code>CAP_NET_RAW</code> capability so that we can use <code>ping</code> inside the container for basic network checks.</p> <pre><code>binchen@m:~/container/runc$ git diff\ndiff --git a/config.json b/config.json\nindex 25a3154..d1c0fb2 100644\n--- a/config.json\n+++ b/config.json\n@@ -18,12 +18,16 @@\n                        \"bounding\": [\n                                \"CAP_AUDIT_WRITE\",\n                                \"CAP_KILL\",\n-                               \"CAP_NET_BIND_SERVICE\"\n+                               \"CAP_NET_BIND_SERVICE\",\n+                               \"CAP_NET_RAW\"\n                        ],\n                        \"effective\": [\n                                \"CAP_AUDIT_WRITE\",\n                                \"CAP_KILL\",\n-                               \"CAP_NET_BIND_SERVICE\"\n+                               \"CAP_NET_BIND_SERVICE\",\n+                               \"CAP_NET_RAW\"\n                        ],\n                        \"inheritable\": [\n                                \"CAP_AUDIT_WRITE\",\n@@ -33,7 +37,9 @@\n                        \"permitted\": [\n                                \"CAP_AUDIT_WRITE\",\n                                \"CAP_KILL\",\n-                               \"CAP_NET_BIND_SERVICE\"\n+                               \"CAP_NET_BIND_SERVICE\",\n+                               \"CAP_NET_RAW\"\n                        ],\n                        \"ambient\": [\n                                \"CAP_AUDIT_WRITE\",\n@@ -131,6 +137,16 @@\n                        ]\n                }\n        ],\n+\n+       \"hooks\":\n+               {\n+                       \"prestart\": [\n+                               {\n+                                       \"path\": \"/usr/local/bin/netns\"\n+                               }\n+                       ]\n+               },\n+\n        \"linux\": {\n                \"resources\": {\n                        \"devices\": [\n</code></pre> <p>start a container with this new config.</p> <p>Inside the container, we find an <code>eth0</code> device, in addition to a <code>loop</code> device that is always there.</p> <pre><code>/ # ifconfig\neth0      Link encap:Ethernet  HWaddr 8E:F3:5C:D8:CA:2B\n          inet addr:172.19.0.2  Bcast:172.19.255.255  Mask:255.255.0.0\n          inet6 addr: fe80::8cf3:5cff:fed8:ca2b/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:21992 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:241 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:2610155 (2.4 MiB)  TX bytes:22406 (21.8 KiB)\n\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:6 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1\n          RX bytes:498 (498.0 B)  TX bytes:498 (498.0 B)\n</code></pre> <p>And, you will be able to ping (*) outside world.</p> <pre><code>/ # ping 216.58.199.68\nPING 216.58.199.68 (216.58.199.68): 56 data bytes\n64 bytes from 216.58.199.68: seq=0 ttl=55 time=18.382 ms\n64 bytes from 216.58.199.68: seq=1 ttl=55 time=17.936 ms\n</code></pre> <p>Notes: 216.58.199.68 is one IP of google.com. If we had set up the DNS namesever (e.g echo nameserver 8.8.8.8 &gt; /etc/resolv.conf), we would have been able to ping www.google.com.</p> <p>So, how it works?</p>"},{"location":"ch7/#bridge-veth-route-and-iptablenat","title":"Bridge, Veth, Route, and iptable/NAT","text":"<p>When a hook is called, the container runtime passes the container's state to the hook. This includes the PID of the container (in the runtime namespace). The hook, <code>Netns</code> in this case, uses this PID to determine the network namespace in which the container is supposed to run. With this PID, <code>netns</code> performs a few tasks:</p> <p>1) It creates a Linux bridge with the default name <code>netns0</code> (if one doesn't already exist). It also sets up the MASQUERADE rule on the host. 2) It creates a veth pair, connects one endpoint of the pair to the bridge <code>netns0</code>, and places the other one (renamed to <code>eth0</code>) into the container's network namespaces. 3) It allocates and assigns an IP to the container interface (<code>eth0</code>) and sets up the Route table for the container.</p> <p>We'll soon delve into the details of the above-mentioned tasks. But first, let's start another container with the same <code>config.json</code>. This should make things clearer and more interesting than having just one container.</p> <ul> <li>bridge and interfaces</li> </ul> <p>A bridge <code>netns0</code> is created and two interfaces are associated with it. The name of the interface follows the format of <code>netnsv0-$(containerPid)</code>.</p> <pre><code>$ brctl show netns0\nbridge name    bridge id        STP enabled    interfaces\nnetns0        8000.f2df1fb10980    no        netnsv0-8179\n                                             netnsv0-10577\n</code></pre> <p>As we explained before <code>netnsv0-8179</code> is one endpoint of the veth pair, connecting to the bridge; the other endpoint is inside of the container 8179. Let's find it out.</p> <ul> <li>veth pair</li> </ul> <p>On the host, we can see the peer of <code>netnsv0-8179</code> is index <code>7</code></p> <pre><code>$ ethtool -S netnsv0-8179\nNIC statistics:\n     peer_ifindex: 7\n</code></pre> <p>And in the container 8179, we can see the eth0's index is 7. It confirms that the <code>eth0</code> in container 8179 is paired with <code>netnsv0-8179</code> in the host. Same is true for <code>netnsv0-10577</code> and the <code>eth0</code> in container 10577.</p> <pre><code>/ # ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue qlen 1000\n    link/ether 8e:f3:5c:d8:ca:2b brd ff:ff:ff:ff:ff:ff\n    inet 172.19.0.2/16 brd 172.19.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::8cf3:5cff:fed8:ca2b/64 scope link\n       valid_lft forever preferred_lft forever\n</code></pre> <p>So far, we have seen how a container is connected to host virtul bridge using veth pair. We have the network interfaces but still need a few more setups: Route table and iptable.</p>"},{"location":"ch7/#route-table","title":"Route Table","text":"<p>Here is the route table for In container <code>8179</code>:</p> <pre><code>/ # route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         172.19.0.1      0.0.0.0         UG    0      0        0 eth0\n172.19.0.0      *               255.255.0.0     U     0      0        0 eth0\n</code></pre> <p>We can see the all traffic will goes through <code>eth0</code> to the gateway, which is the bridge <code>netns0</code>, as shown by:</p> <pre><code># in container\n/ # ip route get 216.58.199.68 from 172.19.0.2\n216.58.199.68 from 172.19.0.2 via 172.19.0.1 dev eth0\n</code></pre> <p>In the host:</p> <pre><code>$ route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         192-168-1-1     0.0.0.0         UG    0      0        0 wlan0\n172.19.0.0      *               255.255.0.0     U     0      0        0 netns0\n192.168.1.0     *               255.255.255.0   U     9      0        0 wlan0\n192.168.122.0   *               255.255.255.0   U     0      0        0 virbr0\n</code></pre> <p>Also:</p> <pre><code># on host\n$ ip route get 216.58.199.68 from 172.19.0.1\n216.58.199.68 from 172.19.0.1 via 192.168.1.1 dev wlan0\n    cache\n</code></pre> <p>The <code>192.168.1.1</code> is the ip of my home route, which is a real bridge.</p> <p>Piece together the route in the container, we can see when ping google from the container, the package will go to the virtual bridge created by the <code>netns</code> first, and then goes to the real route gateway at my home, and then into the wild internet and finally to one of the goole servers.</p>"},{"location":"ch7/#iptablenat","title":"Iptable/NAT","text":"<p>Another change made by the <code>netns</code> is to set up the MASQUERADE target, that means all traffic with a source of <code>172.19.0.0/16</code> will be MASQUERADE or NAT-ed with the host address so that outside can only see the host (ip) but not the container (ip).</p> <pre><code># sudo iptables -t nat --list\nChain POSTROUTING (policy ACCEPT)\ntarget     prot  opt source               destination\nMASQUERADE  all  --  172.19.0.0/16        anywhere\n</code></pre>"},{"location":"ch7/#port-forwarddnat","title":"Port forward/DNAT","text":"<p>With Ip MASQUERADE, the traffic can goes out from the container to the internet as well as the return traffic from the same connection. However, for conatiner to accept incoming connections, you have set up the port forwarding using iptable DNAT target.</p> <p>In container:</p> <pre><code>/ # nc -p 10001 -l\n</code></pre> <p>port map: host:100088 maps to container xyxy12:1024</p> <pre><code>iptables -t nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 100088 -j DNAT --to-destination 172.19.0.8:10001\n</code></pre> <p>host</p> <pre><code>echo the host says HI |  nc localhost 5555\n</code></pre>"},{"location":"ch7/#share-network-namespace","title":"Share network namespace","text":"<p>To join the network namespace of another container, set up the network namespace path pointing to the one you want to join. In our example, we'll join the network namespace of container 8179.</p> <pre><code>{\n-                \"type\": \"network\"\n+                \"type\": \"network\",\n+                \"path\": \"/proc/8179/ns/net\"\n</code></pre> <p>Remember to remove the prestart hook, since we don't need to create a new network interface (veth pair and route table) this time.</p> <p>Start a new container, and we'll find that the new container has the same <code>eth0</code> device (as well as same ip) with the container 8179 and the route table is same as the one in container 8179 since they are in the same network namespace.</p> <pre><code>/ # ifconfig\neth0      Link encap:Ethernet  HWaddr 8E:F3:5C:D8:CA:2B\n          inet addr:172.19.0.2  Bcast:172.19.255.255  Mask:255.255.0.0\n          inet6 addr: fe80::8cf3:5cff:fed8:ca2b/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:22371 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:241 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:2658017 (2.5 MiB)  TX bytes:22406 (21.8 KiB)\n\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:6 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1\n          RX bytes:498 (498.0 B)  TX bytes:498 (498.0 B)\n</code></pre> <p>So, despite being in different containers, they share the same network device, route table, port numbers and all the other network resources. For example, if you start a web service in container 8179 port 8100 and you will be able to access the service in this new container using localhost:8100.</p>"},{"location":"ch7/#summary","title":"Summary","text":"<p>We've seen how to use <code>netns</code> as a hook to set up a bridge network for our containers, enabling them to communicate with the internet and each other. In diagram form, we've set up something like this:</p> <pre><code>+---------------------------------------------------------+\n|                                                         |\n|                                                         |\n|                                      +----------------+ |\n|                                      |   wlan/eth0    +---+\n|                                      |                | |\n|                                      +---------+------+ |\n|                                                |        |\n|                                          +-----+----+   |\n|                                    +-----+route     |   |\n|                                    |     |table     |   |\n|                                    |     +----------+   |\n|    +-------------------------------+----------+         |\n|    |                                          |         |\n|    |                bridge:netns0             |         |\n|    |                                          |         |\n|    +-----+-----------------------+------------+         |\n|          | interface             | interface            |\n|    +-----+-----+          +------+----+                 |\n|    |           |          |10:netnsv0 |                 |\n|    |8:netnsv0- |          +-10577@if9 |                 |\n|    |8179@if7   |          |           |                 |\n|    +---+-------+          +----+------+                 |\n|        |                       |                        |\n|        |                       |                        |\n| +-----------------+     +-----------------+             |\n| |      |          |     |      |          |             |\n| |  +---+------+   |     | +----+------+   |             |\n| |  |          |   |     | |           |   |             |\n| |  |7:eth0@if8|   |     | | 9:eth0@if10   |             |\n| |  |          |   |     | |           |   |             |\n| |  |          |   |     | |           |   |             |\n| |  +----------+   |     | +-----------+   |             |\n| |                 |     |                 |             |\n| |  c8179          |     |  c10577         |             |\n| +-----------------+     +-----------------+             |\n|                                                         |\n+---------------------------------------------------------+\n</code></pre>"},{"location":"ch8/","title":"CNI","text":"<p>CNI, or <code>Container Network Interface</code>, originated from CoreOS as a network solution for rkt, and surpassed Docker's CNM by being adopted by k8s as the network plugin interface.</p> <p>In this blog, we will explore how to use CNI, specifically the bridge plugin, to set up the network for containers spawned by runc, achieving the same result/topology as we did in the last blog using <code>netns</code> as the hook.</p>"},{"location":"ch8/#overview","title":"Overview","text":"<p>The caller/user of CNI (e.g., a container runtime/orchestrator such as runc or k8s) interacts with a plugin using two things: a network configuration file and some environment variables. The configuration file contains the configs of the network (or subnet) the container is supposed to connect to. The environment variables include the path where the plugin binary and network configuration files are located, plus \"add/delete which container to/from which network namespace\". This could be implemented by passing arguments to the plugin (instead of using environment variables). It's not a big issue, but it seems a bit \"unusual\" to use the environment to pass arguments.</p> <p>For a more detailed introduction to CNI, see here and here.</p>"},{"location":"ch8/#use-cni-plugins","title":"Use CNI plugins","text":""},{"location":"ch8/#install-plugins","title":"Install plugins","text":"<pre><code>go get github.com/containernetworking/plugins\ncd $GOPATH/src/github.com/containernetworking/plugins\n./build.sh\nmkdir -p /opt/cni/bin/bridge\nsudo cp bin/* c\n</code></pre>"},{"location":"ch8/#use-cni","title":"Use CNI","text":"<p>We'll be using the following simple script to exercise CNI with runc. It covers all the essential concepts in one place, which is nice.</p> <pre><code>$ cat runc_cni.sh\n#!/bin/sh\n\n# need run with root\n# ADD or DEL or VERSION\naction=$1\ncid=$2\npid=$(runc ps $cid | sed '1d' | awk '{print $2}')\nplugin=/opt/cni/bin/bridge\n\nexport CNI_PATH=/opt/cni/bin/\nexport CNI_IFNAME=eth0\nexport CNI_COMMAND=$action\nexport CNI_CONTAINERID=$cid\nexport CNI_NETNS=/proc/$pid/ns/net\n\n$plugin &lt;&lt;EOF\n{\n    \"cniVersion\": \"0.2.0\",\n    \"name\": \"mynet\",\n    \"type\": \"bridge\",\n    \"bridge\": \"cnibr0\",\n    \"isGateway\": true,\n    \"ipMasq\": true,\n    \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"172.19.1.0/24\",\n        \"routes\": [\n            { \"dst\": \"0.0.0.0/0\" }\n        ],\n     \"dataDir\": \"/run/ipam-state\"\n    },\n    \"dns\": {\n      \"nameservers\": [ \"8.8.8.8\" ]\n    }\n}\nEOF\n</code></pre> <p>It may not be obvious to a newcomer that we are using two plugins here, the bridge plugin and host-local. The former is used to set up a bridge network (as well as a veth pair), and the latter is used to allocate and assign IP addresses to the containers (and the bridge gateway). This is referred to as <code>ipam</code> (IP Address Management), as you might have noticed in the config key.</p> <p>The internal working of the bridge plugging is almost the same as the <code>netns</code> does and we are not going to repeat it here.</p> <p>Start a container called <code>c1</code>, <code>sudo runc run c1</code>.</p> <p>Then, put <code>c1</code> into the network:</p> <pre><code>sudo ./runc_cni.sh ADD c1\n</code></pre> <p>Below is the output, telling you the ip and gateway of <code>c1</code>, among other things.</p> <pre><code>{\n    \"cniVersion\": \"0.2.0\",\n    \"ip4\": {\n        \"ip\": \"172.19.1.6/24\",\n        \"gateway\": \"172.19.1.1\",\n        \"routes\": [\n            {\n                \"dst\": \"0.0.0.0/0\",\n                \"gw\": \"172.19.1.1\"\n            }\n        ]\n    },\n    \"dns\": {\n        \"nameservers\": [\n            \"8.8.8.8\"\n        ]\n    }\n}\n</code></pre> <p>You can create another container <code>c2</code> and add it to the same network in a similar way. Now, we have a subnet with two containers inside. They can communicate with each other and can ping outside IPs, thanks to the route setting and IP masquerade. However, DNS won't work.</p> <p>You can also remove a container from the network. After doing so, the container won't be connected to the bridge anymore.</p> <pre><code>sudo ./runc_cni.sh DEL c1\n</code></pre> <p>However, the IP resource won't be reclaimed automatically, you have to do that \"manually\".</p> <p>That is it, as we said this will be a short ride. Have fun with CNI.</p>"}]}